# SariCoach â€“ Agents Intensive Capstone

SariCoach is a multimodal retail coach for sari-sari stores. It consumes
JSON payloads derived from Odoo 18 CE / Kaggle datasets and returns
structured metrics plus plain-language recommendations.

## ðŸ—ï¸ Technology Stack & Architecture

SariCoach is designed as a **"Pocket Enterprise Intelligence"** layer for micro-retailers, built on a robust open-source foundation:

1.  **ERP Core**: **Odoo 18 CE + OCA** modules provide the transactional bedrock (Inventory, Sales, Purchase).
2.  **Data Layer**: **Supabase** (Postgres) serves as the scalable backend, hosting the canonical data models and exposing them via secure APIs/Edge Functions.
3.  **Agentic Layer**: A multi-agent system orchestrating insights:
    *   **PlannerAgent**: Routes user intent (Analyze Store vs. Explain Brand).
    *   **DataAnalystAgent**: Builds unified feature frames from multimodal signals (Sales + Shelf Vision + STT).
    *   **CoachAgent**: Translates metrics into action. Powered by **Gemini 3** (or Flash 2.0) for advanced reasoning and culturally nuanced coaching.
4.  **Client**: Mobile-first design (simulated here via Notebook/API) to run on low-cost devices.

## Data Pipeline & Synthetic Multimodal Dataset

This project uses a **reproducible data pipeline** to turn Kaggle-style retail data into a
multimodal SariCoach dataset with:

- **Tabular core**: brands, products, stores, transactions, transaction_lines
- **Synthetic signals**: shelf vision (facings, share of shelf, OOS), STT brand mentions,
  daily weather, and foot traffic per store

All of this is generated by a single script:

```bash
cd agents-intensive-saricoach

# 1) Place Kaggle CSVs into data/raw/
#    (e.g. store sales + orders + order_items + products + stores)
ls data/raw/

# 2) Generate canonical SariCoach dataset
python3 seed_saricoach_data.py
```

Running `seed_saricoach_data.py` produces:

* `data/processed/brands.csv`
* `data/processed/products.csv`
* `data/processed/stores.csv`
* `data/processed/transactions.csv`
* `data/processed/transaction_lines.csv`
* `data/processed/shelf_vision_events.csv`
* `data/processed/stt_events.csv`
* `data/processed/weather_daily.csv`
* `data/processed/foot_traffic_daily.csv`
* `data/processed/seed_saricoach.sql` (optional Postgres seed script)

These processed tables are the **canonical contract** the agents use:

* The **DataAnalystAgent** joins transactions, shelf vision, STT, weather, and traffic
  into a unified feature frame per store/brand/day.
* The **CoachAgent** generates plain-language recommendations grounded in those features
  (e.g., â€œlow facings + rain + high STT demand â†’ increase facings / avoid stockoutâ€).

In a real deployment, `seed_saricoach.sql` can be used to seed a Postgres/Supabase
database behind an API or Odoo 18 CE / OCA module. For the capstone, the **Kaggle
notebook reads directly from `data/processed/*.csv`** to ensure everything runs
end-to-end in a single runtime.

> ðŸ” Regenerating the dataset
>
> If you want to change the underlying raw data, replace the CSVs under `data/raw/` and
> re-run:
>
> ```bash
> python3 seed_saricoach_data.py
> ```
>
> All downstream analysis and agent behavior will automatically update to the newly
> generated **multimodal** dataset.

## ðŸš€ Real Backend & Mobile Dashboard

Beyond the notebook demo, this repo includes a **real FastAPI backend** and a **React mobile dashboard**.

### 1. Backend Service

Exposes the agent logic via REST API.

```bash
# Install dependencies
pip install -r service/requirements.txt

# Run server
uvicorn service.app.main:app --reload --port 8000
```

Endpoints:
- `GET /api/store/{id}/summary`: Returns KPIs and Coach advice.
- `POST /api/coach/recommendations`: Get specific advice for a brand/category.

### 2. Mobile Dashboard (React)

A mobile-first web app for store owners.

```bash
cd dashboard
# Install dependencies (requires Node.js)
npm install
# Run dev server
npm run dev
```

Configure API URL in `dashboard/.env.local`:
```
VITE_API_URL=http://localhost:8000
```

```bash
make init
make api
```

Then POST a payload to:

```text
POST http://localhost:8080/analyze/store
{
  "payload": {
    "store": { "id": 1, "name": "Demo Store", "code": "DEMO" },
    "transactions": [ ... ]
  }
}
```

To run the lightweight evaluation harness:

```bash
make eval
```

LLM calls use the Google AI SDK (Gemini). Set:

```bash
export SARICOACH_GOOGLE_API_KEY="YOUR_KEY"
```

If the key is not set, the service falls back to a deterministic stub so the
codebase remains runnable in constrained environments (e.g. CI/Kaggle).
